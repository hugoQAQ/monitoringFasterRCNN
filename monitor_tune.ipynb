{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract validation feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/17 08:52:50 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from model_final_vos_resnet_voc.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  pixel_mean\n",
      "  pixel_std\n",
      "  roi_heads.logistic_regression.{bias, weight}\n",
      "  roi_heads.noise.noise\n",
      "  roi_heads.weight_energy.{bias, weight}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on voc-val\n",
      " 100% |███████████████| 4952/4952 [6.9m elapsed, 0s remaining, 11.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from detectron2.data import get_detection_dataset_dicts\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import argparse\n",
    "from pickle import load\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from vos.detection.modeling.regnet import build_regnet_fpn_backbone\n",
    "# # create a parser object\n",
    "# parser = argparse.ArgumentParser(description='Description of your program')\n",
    "\n",
    "# # add an optional \"--model\" argument \n",
    "# parser.add_argument('--backbone', type=str, help='Description of the backbone argument')\n",
    "# parser.add_argument('--id', type=str, help='Description of the in-distribution dataset argument')\n",
    "# parser.add_argument('--tau', nargs='+', type=float, help='Description of the tau argument')\n",
    "\n",
    "# # %%\n",
    "# args = parser.parse_args()\n",
    "\n",
    "id = \"voc\"\n",
    "backbone = \"resnet\"\n",
    "taus = [0.05]\n",
    "\n",
    "VOC_THING_CLASSES = ['person',\n",
    "                     'bird',\n",
    "                     'cat',\n",
    "                     'cow',\n",
    "                     'dog',\n",
    "                     'horse',\n",
    "                     'sheep',\n",
    "                     'airplane',\n",
    "                     'bicycle',\n",
    "                     'boat',\n",
    "                     'bus',\n",
    "                     'car',\n",
    "                     'motorcycle',\n",
    "                     'train',\n",
    "                     'bottle',\n",
    "                     'chair',\n",
    "                     'dining table',\n",
    "                     'potted plant',\n",
    "                     'couch',\n",
    "                     'tv',\n",
    "                     ]\n",
    "\n",
    "BDD_THING_CLASSES = ['pedestrian',\n",
    "                    'rider',\n",
    "                    'car',\n",
    "                    'truck',\n",
    "                    'bus',\n",
    "                    'train',\n",
    "                    'motorcycle',\n",
    "                    'bicycle',\n",
    "                    'traffic light',\n",
    "                    'traffic sign']\n",
    "label_list = VOC_THING_CLASSES if id == 'voc' else BDD_THING_CLASSES\n",
    "label_dict = {i:label for i, label in enumerate(label_list)}\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(f\"vos/detection/configs/BDD-Detection/faster-rcnn/{backbone}.yaml\")\n",
    "cfg.MODEL.WEIGHTS = f\"model_final_vos_{backbone}_{id}.pth\" \n",
    "cfg.MODEL.DEVICE='cuda'\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(label_list)\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "aug = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# %%\n",
    "def inference(inputs):\n",
    "    with torch.no_grad():\n",
    "        images = model.preprocess_image(inputs)  \n",
    "        feats = model.backbone(images.tensor)  \n",
    "        proposals, _ = model.proposal_generator(images, feats, None)  # RPN\n",
    "\n",
    "        features_ = [feats[f] for f in model.roi_heads.box_in_features]\n",
    "        box_features = model.roi_heads.box_pooler(features_, [x.proposal_boxes for x in proposals])\n",
    "        box_features = model.roi_heads.box_head(box_features)  # features of all 1k candidates\n",
    "        predictions = model.roi_heads.box_predictor(box_features)\n",
    "        pred_instances, pred_inds = model.roi_heads.box_predictor.inference(predictions, proposals)\n",
    "        pred_instances = model.roi_heads.forward_with_given_boxes(feats, pred_instances)\n",
    "\n",
    "        # output boxes, masks, scores, etc\n",
    "        pred_instances = model._postprocess(pred_instances, inputs, images.image_sizes)  # scale box to orig size\n",
    "        # features of the proposed boxes\n",
    "        feats = box_features[pred_inds]\n",
    "    return pred_instances, feats\n",
    "\n",
    "# %%\n",
    "feats_list = []\n",
    "for tau in taus:\n",
    "    # print(f\"tau: {tau}\")\n",
    "    monitors_dict = {}\n",
    "    for class_name in label_list:\n",
    "        monitor_path = f\"monitors/{id}/{backbone}/{class_name}/monitor_for_clustering_parameter\" + \"_tau_\" + str(tau) + \".pkl\"\n",
    "        # monitor_path = f\"Monitors/{class_name}/monitor_for_clustering_parameter\" + \"_tau_\" + str(tau) + \".pkl\"\n",
    "        if os.path.exists(monitor_path):\n",
    "            with open(monitor_path, 'rb') as f:\n",
    "                monitor = load(f)\n",
    "            for i in range(len(monitor.good_ref)):\n",
    "                monitor.good_ref[i].ivals = monitor.good_ref[i].ivals*np.array([1-delta, 1+delta])\n",
    "            monitors_dict[class_name] = monitor\n",
    "        else:\n",
    "            print(f\"monitor for {class_name} not found\")\n",
    "    eval_list = [\"bdd-val\", \"ID-bdd-OOD-coco\", \"OOD-open\"] if id == \"bdd\" else [\"voc-val\", \"ID-voc-OOD-coco\", \"OOD-open\"]\n",
    "    # for dataset_name in eval_list:\n",
    "    for dataset_name in [\"voc-val\"]:\n",
    "        print(f\"evaluating on {dataset_name}\")\n",
    "        dataset_val = fo.load_dataset(dataset_name)\n",
    "        i = 0\n",
    "        feats_list = []\n",
    "        with fo.ProgressBar() as pb:\n",
    "            for sample in pb(dataset_val):\n",
    "                image = cv2.imread(sample.filepath)\n",
    "                height, width = image.shape[:2]\n",
    "                image = aug.get_transform(image).apply_image(image)\n",
    "                image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1)).to(device)\n",
    "                inputs = [{\"image\": image, \"height\": height, \"width\": width}]\n",
    "                preds, feats = inference(inputs)\n",
    "                feats = feats.cpu().detach().numpy()\n",
    "                boxes = preds[0][\"instances\"].get(\"pred_boxes\").tensor.cpu().detach().numpy()\n",
    "                scores = preds[0][\"instances\"].get(\"scores\").cpu().detach().numpy()\n",
    "                classes = preds[0][\"instances\"].pred_classes.cpu().detach().numpy()\n",
    "                \n",
    "                detections = []\n",
    "                oods = []             \n",
    "                for label, score, box, feat in zip(classes, scores, boxes, feats):\n",
    "                    label = label_dict[label]\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rel_box = [x1/width, y1/height, (x2 - x1) / width, (y2 - y1) / height]\n",
    "                    verdict = monitors_dict[label].make_verdicts(feat[np.newaxis, :])[0]\n",
    "                    detections.append(\n",
    "                        fo.Detection(\n",
    "                            label=label,\n",
    "                            bounding_box=rel_box,\n",
    "                            confidence=score,\n",
    "                            verdict=verdict,\n",
    "                            feature_idx=i\n",
    "                        ),\n",
    "                    )\n",
    "                    i += 1\n",
    "                    feats_list.append(feat)\n",
    "                #     if not verdict:\n",
    "                #         oods.append(\n",
    "                #             fo.Detection(\n",
    "                #                 label=\"OOD\",\n",
    "                #                 bounding_box=rel_box\n",
    "                #             ),\n",
    "                #         )\n",
    "                # sample[\"OOD\"] = fo.Detections(detections=oods)\n",
    "                sample[\"prediction\"] = fo.Detections(detections=detections)\n",
    "                sample.save()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████| 4952/4952 [34.1s elapsed, 0s remaining, 151.0 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 4952/4952 [27.0s elapsed, 0s remaining, 198.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "results = dataset_val.evaluate_detections(\n",
    "\"prediction\",\n",
    "gt_field=\"detections\",\n",
    "eval_key=\"eval\",\n",
    "compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 17479 features\n"
     ]
    }
   ],
   "source": [
    "feats_npy = np.array(feats_list)\n",
    "print(f\"Saved {feats_npy.shape[0]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 4811/4811 [9.7s elapsed, 0s remaining, 561.9 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "tp_prediction_view = dataset_val.filter_labels(\"prediction\", F(\"eval\") == \"tp\")\n",
    "class_names = list(label_dict.values())\n",
    "features_idx_dict = {cls:[] for cls in class_names}\n",
    "with fo.ProgressBar() as pb:\n",
    "    for sample in pb(tp_prediction_view):\n",
    "        for detection in sample.prediction.detections:\n",
    "            label_pred = detection.label\n",
    "            feature_idx = detection.feature_idx\n",
    "            features_idx_dict[label_pred].append(feature_idx)\n",
    "feats_tp_dict = {cls:feats_npy[features_idx_dict[cls]] for cls in class_names}\n",
    "sum = 0\n",
    "for k,v in features_idx_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "    sum += len(v)\n",
    "print(f\"total: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2640/2640 [5.4s elapsed, 0s remaining, 506.6 samples/s]      \n",
      "person: 1869\n",
      "bird: 103\n",
      "cat: 89\n",
      "cow: 119\n",
      "dog: 177\n",
      "horse: 114\n",
      "sheep: 85\n",
      "airplane: 67\n",
      "bicycle: 115\n",
      "boat: 189\n",
      "bus: 69\n",
      "car: 502\n",
      "motorcycle: 102\n",
      "train: 121\n",
      "bottle: 275\n",
      "chair: 705\n",
      "dining table: 221\n",
      "potted plant: 314\n",
      "couch: 238\n",
      "tv: 138\n",
      "total: 5612\n"
     ]
    }
   ],
   "source": [
    "fp_prediction_view = dataset_val.filter_labels(\"prediction\", F(\"eval\") == \"fp\")\n",
    "class_names = list(label_dict.values())\n",
    "features_idx_dict = {cls:[] for cls in class_names}\n",
    "with fo.ProgressBar() as pb:\n",
    "    for sample in pb(fp_prediction_view):\n",
    "        for detection in sample.prediction.detections:\n",
    "            label_pred = detection.label\n",
    "            feature_idx = detection.feature_idx\n",
    "            features_idx_dict[label_pred].append(feature_idx)\n",
    "feats_fp_dict = {cls:feats_npy[features_idx_dict[cls]] for cls in class_names}\n",
    "sum = 0\n",
    "for k,v in features_idx_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "    sum += len(v)\n",
    "print(f\"total: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{id}_feats_tp_dict.pickle', 'wb') as f:\n",
    "    # dump the data into the file using pickle.dump()\n",
    "    pickle.dump(features_tp_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{id}_feats_fp_dict.pickle', 'wb') as f:\n",
    "    # dump the data into the file using pickle.dump()\n",
    "    pickle.dump(features_fp_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monitor evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import os\n",
    "def load_monitors(id, backbone, label_list, tau_list):\n",
    "    monitors_dict = dict()\n",
    "    for class_name, tau in zip(label_list, tau_list):\n",
    "        monitor_path = f\"monitors/{id}/{backbone}/{class_name}/monitor_for_clustering_parameter\" + \"_tau_\" + str(tau) + \".pkl\"\n",
    "        # monitor_path = f\"Monitors/{class_name}/monitor_for_clustering_parameter\" + \"_tau_\" + str(tau) + \".pkl\"\n",
    "        if os.path.exists(monitor_path):\n",
    "            with open(monitor_path, 'rb') as f:\n",
    "                monitor = load(f)\n",
    "            monitors_dict[class_name] = monitor\n",
    "        else:\n",
    "            print(f\"monitor for {monitor_path} not found\")\n",
    "    return monitors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gradio as gr\n",
    "VOC_THING_CLASSES = ['person',\n",
    "                        'bird',\n",
    "                        'cat',\n",
    "                        'cow',\n",
    "                        'dog',\n",
    "                        'horse',\n",
    "                        'sheep',\n",
    "                        'airplane',\n",
    "                        'bicycle',\n",
    "                        'boat',\n",
    "                        'bus',\n",
    "                        'car',\n",
    "                        'motorcycle',\n",
    "                        'train',\n",
    "                        'bottle',\n",
    "                        'chair',\n",
    "                        'dining table',\n",
    "                        'potted plant',\n",
    "                        'couch',\n",
    "                        'tv',\n",
    "                        ]\n",
    "\n",
    "BDD_THING_CLASSES = ['pedestrian',\n",
    "                    'rider',\n",
    "                    'car',\n",
    "                    'truck',\n",
    "                    'bus',\n",
    "                    # 'train',\n",
    "                    'motorcycle',\n",
    "                    'bicycle',\n",
    "                    'traffic light',\n",
    "                    'traffic sign']\n",
    "benchmark_vos = {\"voc\": {\"resnet\":[47.53, 51.33], \"regnet\":[47.77, 48.33]}, \"bdd\": {\"resnet\":[44.27, 35.54], \"regnet\":[36.61, 27.24]}}\n",
    "\n",
    "# def tune_parameter(id, backbone, tau, delta, num_limit, progress=gr.Progress()):\n",
    "def tune_parameter(id, backbone, tau, delta, num_limit):\n",
    "    # benchmark\n",
    "    benchmark = benchmark_vos[id][backbone]\n",
    "    # load feats\n",
    "    label_list = VOC_THING_CLASSES if id == 'voc' else BDD_THING_CLASSES\n",
    "    dataset_name = \"voc-val\" if id == \"voc\" else \"bdd-val\"\n",
    "    with open(f'eval_feats/{id}/{backbone}/{dataset_name}_feats_tp_dict.pickle', 'rb') as f:\n",
    "        feats_tp_dict = pickle.load(f)\n",
    "    with open(f'eval_feats/{id}/{backbone}/{dataset_name}_feats_fp_dict.pickle', 'rb') as f:\n",
    "        feats_fp_dict = pickle.load(f)\n",
    "    \n",
    "    # load monitors\n",
    "    tau_list = [tau]*len(label_list)\n",
    "    if id == \"voc\":\n",
    "        for i in [label_list.index(\"person\"), label_list.index(\"car\")]:\n",
    "            tau_list[i] = 0.05\n",
    "    monitors_dict = load_monitors(id, backbone, label_list, tau_list)\n",
    "    for label in label_list:\n",
    "        if len(feats_tp_dict[label]) < num_limit:\n",
    "            for i in range(len(monitors_dict[label].good_ref)):\n",
    "                monitors_dict[label].good_ref[i].ivals = monitors_dict[label].good_ref[i].ivals*np.array([1-delta, 1+delta])\n",
    "    data_num_clusters = [[label, len(monitors_dict[label].good_ref)] for label in label_list]\n",
    "    \n",
    "    # make verdicts on ID data\n",
    "    data_tp = []\n",
    "    data_fp = []\n",
    "    accept_sum = {\"tp\": 0, \"fp\": 0}\n",
    "    reject_sum = {\"tp\": 0, \"fp\": 0}\n",
    "    # progress(0, desc=\"Starting\")\n",
    "    # for label in progress.tqdm(label_list, desc=\"Evaluation on ID data\"):    \n",
    "    for label in label_list:  \n",
    "        verdict = monitors_dict[label].make_verdicts(feats_tp_dict[label])\n",
    "        data_tp.append([label, len(verdict), np.sum(verdict)/len(verdict)])\n",
    "        accept_sum[\"tp\"] += np.sum(verdict)\n",
    "        reject_sum[\"tp\"] += len(verdict) - np.sum(verdict)   \n",
    "        verdict = monitors_dict[label].make_verdicts(feats_fp_dict[label])\n",
    "        data_fp.append([label, len(verdict), (len(verdict)-np.sum(verdict))/len(verdict)])\n",
    "        accept_sum[\"fp\"] += np.sum(verdict)\n",
    "        reject_sum[\"fp\"] += len(verdict) - np.sum(verdict)\n",
    "    TPR = round((accept_sum['tp'] / (reject_sum['tp'] + accept_sum['tp'])*100), 2)\n",
    "    FPR =  round((accept_sum['fp'] / (reject_sum['fp'] + accept_sum['fp'])*100), 2)\n",
    "    dataset_name = \"PASCAL-VOC\" if id == \"voc\" else \"BDD100k\"\n",
    "    df_summary = pd.DataFrame([[dataset_name, f\"{TPR}%\", \"95%\", f\"{FPR}%\"]], columns=[\"Dataset\", \"TPR\", \"TPR(benchmark)\", \"FPR\"])\n",
    "    \n",
    "    data_ood = []\n",
    "    eval_list = [\"ID-voc-OOD-coco\", \"OOD-open\"] if id == \"voc\" else [\"ID-bdd-OOD-coco\", \"OOD-open\"]\n",
    "    i = 0\n",
    "    # for dataset_name in progress.tqdm(eval_list, desc=\"Evaluation on OOD data\"):\n",
    "    for dataset_name in eval_list: \n",
    "        accept_sum = {\"tp\": 0, \"fp\": 0}\n",
    "        reject_sum = {\"tp\": 0, \"fp\": 0}\n",
    "        with open(f'eval_feats/{id}/{backbone}/{dataset_name}_feats_fp_dict.pickle', 'rb') as f:\n",
    "            feats_fp_dict = pickle.load(f)\n",
    "        for label in label_list:\n",
    "            verdict = monitors_dict[label].make_verdicts(feats_fp_dict[label])\n",
    "            # data_ood.append([label, len(verdict), (len(verdict)-np.sum(verdict)), (len(verdict)-np.sum(verdict))/len(verdict)])\n",
    "            accept_sum[\"fp\"] += np.sum(verdict)\n",
    "            reject_sum[\"fp\"] += len(verdict) - np.sum(verdict)\n",
    "        FPR =  round((accept_sum['fp'] / (reject_sum['fp'] + accept_sum['fp'])*100), 2)\n",
    "        # data_ood.append([dataset_name, accept_sum['fp'], reject_sum['fp'], (reject_sum['fp'] + accept_sum['fp']), str(FPR)+\"%\", benchmark[i]])\n",
    "        data_ood.append([dataset_name, str(FPR)+\"%\", str(benchmark[i])+\"%\"])\n",
    "        i += 1\n",
    "    \n",
    "    # prepare dataframes\n",
    "    # df_ood = pd.DataFrame(data_ood, columns=[\"class\", \"accepted FP\", \"rejected FP\", \"Total num.\", \"FPR\", \"FPR(benchmark)\"])\n",
    "    df_ood = pd.DataFrame(data_ood, columns=[\"Dataset\", \"FPR\", \"FPR(benchmark)\"])\n",
    "    df_ood[\"Dataset\"] = [\"COCO\", \"Open Images\"]\n",
    "    df_tp = pd.DataFrame(data_tp, columns=[\"class\", \"number\", \"TP acceptance\"])\n",
    "    df_tp.sort_values(by=[\"TP acceptance\"], ascending=False, inplace=True)\n",
    "    df_tp[\"TP acceptance\"] = df_tp[\"TP acceptance\"].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    df_fp = pd.DataFrame(data_fp, columns=[\"class\", \"number\", \"FP rejection rate\"])\n",
    "    df_fp.sort_values(by=[\"FP rejection rate\"], ascending=False, inplace=True)\n",
    "    df_fp[\"FP rejection rate\"] = df_fp[\"FP rejection rate\"].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    df_clusters = pd.DataFrame(data_num_clusters, columns=[\"class\", \"number of clusters\"])\n",
    "    return df_summary, df_ood, df_tp, df_fp, df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[[\"voc\", \"resnet\", 0.05, 0, 100000], [\"bdd\", \"resnet\", 0.05, 0, 100000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:185: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:188: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:219: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:222: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:89: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:93: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/hugo/miniconda3/lib/python3.8/site-packages/gradio/inputs.py:62: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(value=default, label=label, optional=optional)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching examples at: '/home/hugo/bdd100k-monitoring/gradio_cached_examples/74'\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=\"gradio/monochrome\") as demo:\n",
    "    gr.Markdown(\"# Monitor intervals Tuning App\")\n",
    "    gr.Markdown(\"This app is used to tune the monitor intervals to achieve the desired TPR of 95%\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            id = gr.inputs.Radio([\"voc\", \"bdd\"], label=\"In-distribution dataset\")\n",
    "            backbone = gr.inputs.Radio([\"resnet\", \"regnet\"], label=\"Backbone\")\n",
    "            tau_dropdown = gr.inputs.Dropdown(choices=[1.0, 0.1, 0.05, 0.01], label=\"Tau\")\n",
    "            delta_slider = gr.inputs.Slider(minimum=0, maximum=19, default=0.5, label=\"Delta\")\n",
    "            number_limit_number = gr.inputs.Number(default=100000, label=\"Number limit\")\n",
    "            button = gr.Button(\"Run\")\n",
    "        with gr.Column():\n",
    "            with gr.Tab(\"Monitor Performance\"):\n",
    "                id_table_output = gr.Dataframe(label=\"In-distribution Monitor Performance\")\n",
    "                ood_table_output = gr.Dataframe(label=\"Out-of-distribution Monitor Performance\")\n",
    "            with gr.Tab(\"TP Acceptance\"):\n",
    "                TP_table_output = gr.Dataframe(label=\"TP Acceptance\")\n",
    "            with gr.Tab(\"FP Rejection\"):\n",
    "                FP_table_output = gr.Dataframe(label=\"FP Rejection\")\n",
    "            with gr.Tab(\"Number of clusters\"):\n",
    "                num_clusters_table_output = gr.Dataframe(label=\"Number of clusters\")\n",
    "    examples_block = gr.Examples(inputs=[id, backbone, tau_dropdown, delta_slider, number_limit_number], examples=examples, fn=tune_parameter, outputs=[id_table_output, ood_table_output, TP_table_output, FP_table_output, num_clusters_table_output], cache_examples=True)\n",
    "    button.click(fn=tune_parameter, \n",
    "                                inputs=[id, backbone, tau_dropdown, delta_slider, number_limit_number], \n",
    "                                outputs=[id_table_output, ood_table_output, TP_table_output, FP_table_output, num_clusters_table_output])\n",
    "demo.queue(concurrency_count=10).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
